# ~/blt/train_my_entropy_model.yaml

# Set the output directory for checkpoints
dump_dir: "models/my_entropy_model_checkpoints"
name: "my-custom-entropy-model"

# Total training steps. 100k is a reasonable start. Monitor loss to see if more are needed.
steps: 100000
seed: 42

optim:
  lr: 4e-04
  warmup: 2000

distributed:
  fsdp_type: full_shard
  model_dtype: bf16

# --- Key settings for entropy model training ---
train_entropy_model: true # This flag is crucial.
model: null
# ---------------------------------------------

# --- Entropy Model Architecture (100M-scale) ---
# This is a small, efficient transformer that learns byte-level patterns.
entropy_model:
  dim: 512
  n_layers: 14
  n_heads: 8
  max_seqlen: 512            # The context window ofthe model in bytes.
  vocab_size: 260            # Fixed for byte-level models. Do not change.
  sliding_window: 512        # Use sliding window attention for efficiency.
  attn_bias_type: "local_block_causal"
  attn_impl: "xformers"
# --------------------------------

# --- Data Configuration ---
data:
  root_dir: "data"
  sources:
    my_dataset_raw_chunks: 1.0 # This must match the name of your chunk folder.

  # This tells the dataloader to read your raw .jsonl files directly.
  file_format: "json"

  batch_size: 32             # Adjust based on your GPU memory.
  seq_len: 512               # Must match entropy_model.max_seqlen.
  max_encoder_seq_length: 512  # Must also match.

  add_patches: false         # Entropy model does not use patching.
  patcher_args:
    patching_mode: byte      # Dataloader treats each byte as a patch of size 1.
  tokenizer_args:
    name: blt                # Use the byte-level tokenizer.

checkpoint:
  dump:
    every: 5000              # Save a checkpoint every 5000 steps.
    keep: 2                  # Keep the 2 most recent checkpoints.
  eval:
    every: -1                # Disable evaluation during this training run. 
